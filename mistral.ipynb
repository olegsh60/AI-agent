{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/olegsh60/AI-agent/blob/main/mistral_fixed.ipynb\" target=\"_blank\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYUMj1LZgSCE"
      },
      "source": [
        "## Что делает весь код.\n",
        "\t1.\tУстанавливает библиотеки\n",
        "\t2.\tЗагружает большую LLM с 4-битной квантизацией\n",
        "\t3.\tПринимает текст-подсказку\n",
        "\t4.\tГенерирует продолжение текста\n",
        "\t5.\tПоказывает результат"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMSwxVY7gwD2"
      },
      "source": [
        " ## 1. Установка библиотек\n",
        "\t•\ttransformers — основная библиотека Hugging Face для загрузки моделей.\n",
        "\t•\tbitsandbytes — библиотека для низкоуровневой квантизации моделей (4-bit, 8-bit).\n",
        "\t•\taccelerate — упрощает использование устройств (CPU, GPU) и распределённого запуска."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KilWTRZu01An",
        "outputId": "59758484-b949-4704-b32c-73b8cac5e255"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hFound existing installation: bitsandbytes 0.45.5\n",
            "Uninstalling bitsandbytes-0.45.5:\n",
            "  Successfully uninstalled bitsandbytes-0.45.5\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m311.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.45.5\n"
          ]
        }
      ],
      "source": [
        "# Установка нужных библиотек\n",
        "!pip install -q bitsandbytes transformers accelerate\n",
        "!pip uninstall -y bitsandbytes\n",
        "!pip install bitsandbytes --prefer-binary --no-cache-dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4UkxOl5i_0P"
      },
      "source": [
        "## 2. Импорт библиотек\n",
        "\t•\ttorch — используется для работы с тензорами и устройства (cuda, cpu).\n",
        "\t•\tAutoModelForCausalLM — загрузка модели для генерации текста (CausalLM = causal language modeling).\n",
        "\t•\tAutoTokenizer — преобразует текст в токены и обратно.\n",
        "\t•\tBitsAndBytesConfig — конфигурация для квантизации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaEUQJIt1iB0"
      },
      "outputs": [],
      "source": [
        "# Импорт\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkyRJcvo3ItC"
      },
      "source": [
        "## Аппаратный ускоритель: GPU (проверка)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TN11cxjy3EaX",
        "outputId": "b7f138ff-5c9a-43bc-f8c0-8a1d4e051f28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed May 14 06:44:08 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgpSpBockpZJ"
      },
      "source": [
        "## 3. Настройка 4-битной квантизации\n",
        "Эта конфигурация позволяет сильно уменьшить объём памяти, занимаемый моделью, и ускорить работу на GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agvLduuY1sIs"
      },
      "outputs": [],
      "source": [
        "# Конфигурация квантизации (4-bit)\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml_oY-ysld57"
      },
      "source": [
        "Загружается 7-миллиардная языковая модель Mistral, обученная OpenOrca. Она автоматически размещается на доступном устройстве и работает с 4-битной квантизацией."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "9ulKp1ag1yPX",
        "outputId": "7996ecef-869d-4b6f-f558-ae3daa4382aa"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-6e5e854e15f0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Open-Orca/Mistral-7B-OpenOrca\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    572\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4228\u001b[0;31m             hf_quantizer.validate_environment(\n\u001b[0m\u001b[1;32m   4229\u001b[0m                 \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4230\u001b[0m                 \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             )\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_bitsandbytes_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m     77\u001b[0m                 \u001b[0;34m\"Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             )\n",
            "\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Загрузка модели с автоопределением устройства\n",
        "model_name = \"Open-Orca/Mistral-7B-OpenOrca\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    quantization_config=quant_config\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHQmNWmm5woY"
      },
      "source": [
        "## ❗ Если не работает:\n",
        "\n",
        "Возможно, тебе выдана Colab-среда без GPU (в бесплатном доступе это бывает часто). Тогда:  \n",
        "\t•\tЛибо попробуй перезапустить среду несколько раз.  \n",
        "\t•\tЛибо оформи подписку на Colab Pro, чтобы получать T4/A100 стабильно.  \n",
        "\t•\tЛибо не используй bitsandbytes: загрузи модель в float16, но без 4-bit квантования\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeeKV-K9h5JL"
      },
      "source": [
        "## Пример fallback-кода (без quantization):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259,
          "referenced_widgets": [
            "07d2d860e2c840beb159718dbffa1eba",
            "8bd3c2031dc74b32a2cb357487c852da",
            "0e985b2f80a0429a94c5668aae21c1ad",
            "f2d3a10f70d94ded9873786bf8959934",
            "ae9af952567943028368122ca7a829f5",
            "36598079c7194f0cad77248a527b56b7",
            "8078f161842f48858a462555fe3325b4",
            "4ab98eff4a7d4b03b9310546916b5822",
            "f25c8330342a43958ef7d9b52a5fd997",
            "f794574c9ac14063af621973e4c56f90",
            "f61b7e9ae6d44dbebd52b81e66d88a9d",
            "f09471f7f27347b797d92755caa0252b",
            "06c6462743bf4d149ac981743912059f",
            "6b9ca20f352d4facab184a59b7bd21b9",
            "d893389d89a34989aaedcbad91815670",
            "bc3dda32e0604c04beb1cdc3ab523949",
            "de50fd5578904eedb8946c53b9528db0",
            "c383f8c057a54f72bab713fc955da2c7",
            "342e6445eea24fd9b849ac22f7fad6de",
            "7a50581aa22b4fe3b3366d5222ce5691",
            "5de34b48d3754b27a72e1e7b881df517",
            "f21fdb810f1449068dff6b0dd0648d8d",
            "8a2c493f1d53402caa53b8846f018b19",
            "9194f1ade60e4aaf804a9d3c8a940d70",
            "fad8c2368e47461e8aecd27ec14b35ee",
            "d053cc214acc44b2a5e69993078cfe95",
            "238daab367024344a24acbbc43055453",
            "ee368aec0c1548b3aca190cf08aef224",
            "edf6d93e4dd540509ddeef858e64c44f",
            "c370bd53cb1f46b39f6218b8a587bd39",
            "51b69aa772194ee7aa15a82970c3892b",
            "19f732dc10b74f67be2e962036e10b3b",
            "57266c1deccf45cbab5167ddfbc25cd8",
            "dadda687a9784169b84fd4245c5d9bbd",
            "16022b01e6ce4ee3910057d7dfd8ea76",
            "f261a8d31b6442ea9e29be45eb19bba5",
            "96c3f55366dd4053bf061e76449e2b71",
            "fc277157aab24de686b2cc742ce70b16",
            "5e27e6e7879e4efdb563a3b5f7e3ae11",
            "cb0216a42a83416b9bb5023d8ae4047e",
            "4561eafa19494b6eafd19ff489bc9afe",
            "f26a1136f14f425388fb4abd6bb01ba4",
            "1888f81a698f48c68fe7b358aee5ec59",
            "a081c42efe184fdea30619a4a35dc76d",
            "789b7d7995d1406a9c15986ab53f6c7f",
            "fead29991ba046cea4ad326b83357c68",
            "d88b564475d840a6b15083a6ab607983",
            "52786faf31a643e683b0ed2e32d9df13",
            "ac3a80105dd54e4f9cd53c5fd95987da",
            "d895725ef6174e09ab8041b06ba796a3",
            "050db311adad4b4a8a4dd86a23127ada",
            "054e85004e0f42eb9a537a691df7127d",
            "06bf045486f64c9582aa074d3f9119e8",
            "2749387037ce407090e210d1ae9e78ac",
            "915bc9c297514ad98c6f3f681e1519a9",
            "7b2342cd5ebc4d98889f45770910f7f1",
            "a1b0a1f4104f40b98ffdd6dbe700e0ee",
            "305eda8e3dea47fa8a089f4843222d5c",
            "8067fc5bbb444f1087ca4fc639a2651f",
            "cc6298416b424844be68e94c295084bb",
            "520ddf08d0b64c359b87d983dd82eda8",
            "1cf4472e91a74159bc16d3fb76d06716",
            "c8647d4c6c9f4629a5ec1a5a0e6b0761",
            "a9693b5a495d4bf2815629bbc7b36168",
            "878cd68e6a9e4f9bb8bc07af0e9e6f32",
            "7868045763c246da908688aa23177619",
            "1b3742faf38b4e0f9d8dc1a41e17887d",
            "efdddafce25c4c4f9cae6fd3c2a0871b",
            "ce3993323cef4b868e972796696822f4",
            "fc66ff6816344fe28b011e265277efe5",
            "c911fc2c0ba4487a9f7115ce3d248e0f",
            "00901219347d4f25848402fb84bb89b3",
            "0825f8b3be844eb09074aa4694e69008",
            "51eb8114e8f542119c08c273963fb39b",
            "3506dc8a605e4317adf83de3e0db52a3",
            "1314d02908cd4f03ab7ad45b1cb3cf1b",
            "10e0a3c2bf8d45e18ca31dd1f7b56f4f"
          ]
        },
        "id": "PpoiVtrj5ymX",
        "outputId": "ce92f3d1-12de-4cae-d0ca-bb164b1a0e78"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "07d2d860e2c840beb159718dbffa1eba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f09471f7f27347b797d92755caa0252b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a2c493f1d53402caa53b8846f018b19",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dadda687a9784169b84fd4245c5d9bbd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "789b7d7995d1406a9c15986ab53f6c7f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b2342cd5ebc4d98889f45770910f7f1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b3742faf38b4e0f9d8dc1a41e17887d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/120 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Open-Orca/Mistral-7B-OpenOrca\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16  # Без BitsAndBytesConfig\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca6b3pf6mheu"
      },
      "source": [
        "## 5. Загрузка токенизатора\n",
        "Токенизатор преобразует текст в числовой формат (токены), который понимает модель."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "921674695ff542a98d603af0dd768330",
            "b846f9b7039546389c00970a7ef7e2ef",
            "69c5bd9402e64a41aa1436c10a2720ae",
            "66e8b9501936439d8c3a3074dd5f1288",
            "56b3e4429a314213a2a68272eaca3f07",
            "116f27eea5f64e019145341b18704654",
            "332800464ad64415be00479a3a10674b",
            "842fd1ca6692483f9a7698b01b321cac",
            "1e11032e592442eb93c6f8a84575d956",
            "9c6b3a9f496f45c3a6f8e5307d7da401",
            "ac95fdda94054651b93808d412d6d29b",
            "1ee01285069e442cb120fe7defa74b3c",
            "14bc9dfa26434540b5f0d845bf60d347",
            "811b77090ada40919f0c8ea932ec0828",
            "245c17cc2646474994c58effc4acd87c",
            "07b493672bb546f18ecb7ccf1fb8d583",
            "0ed7ce260bfb46b5bbde0cf906077d9d",
            "72e3add03c43475e9aeba0e46371d013",
            "c838891fce384f33b78f0410bb53a1eb",
            "4c665d92928548498b315eb1d6daf891",
            "378416e529e74620ba5158f6e60cd3a7",
            "50d697af1399487dbc7aed983bb1d16a",
            "0007050957824aa699597752972a6ed6",
            "f280e86dc5914c1bbf3749e3691e09a6",
            "efe94c9df4ec4728a5db0eff4f1a0f8a",
            "b047e01ba2b24746bc2f83d860fd7600",
            "f08af94d4b2c4b91b24cc904b66d54f5",
            "e0311934371344a8b60d31631a276312",
            "d4f23e42ef074c0db9034bd540e270a5",
            "d46d7da6f2f944d88b31142947e92fcc",
            "471965a6b761472aad37126e0b945f1c",
            "cca3d4bc0b5f452795bd5f72c4e92561",
            "a0bed7d13ed44372ae813f6f7d0b6a6f",
            "d0a843d73ab4415fb3b99a2c10aba4c8",
            "1fe2d0f753e640088840262a54700ef7",
            "b70839c5a1e545a28028a3651603df41",
            "63e85d8900934ba8bd1ec86039ab022c",
            "3f05f9c965364a839583d2e963ae2e8f",
            "4fe5259b05f046c59f9d1a6964aa2a87",
            "0be0fc69d83b4aadb47072a20f41278e",
            "9b09e67b08f046e9888313dfbec929f9",
            "3e935e18f91a4062ba04d0983e0662d6",
            "325851d7e9d04d4aa4cfe17859383d61",
            "b9ae7bc8fb7b4278bf643e43412bc7b3"
          ]
        },
        "id": "xhAUwHh72Dic",
        "outputId": "ae4e8ee9-bcd5-4af3-f2d1-7ca4e4379f2b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "921674695ff542a98d603af0dd768330",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.69k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ee01285069e442cb120fe7defa74b3c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0007050957824aa699597752972a6ed6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d0a843d73ab4415fb3b99a2c10aba4c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/101 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Токенизатор\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClO-NryAmufh"
      },
      "source": [
        "## 6. Подготовка входного текста\n",
        "\t•\tТекст преобразуется в токены\n",
        "\t•\t.to(\"cuda\") — отправляется на GPU (если доступен)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZiMoVbknX0I"
      },
      "source": [
        "## 7. Генерация текста\n",
        "\n",
        "\t•\tРезультат переводится обратно в читаемый текст\n",
        "\t•\tУбираются специальные токены (например, <pad>, <eos>)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vI2OYJj2JPb",
        "outputId": "31186ceb-262a-4aa3-ddb4-fbf22c291278"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Расскажи интересный факт о квантовой физике.\n",
            "\n",
            "Квантовая физика - это наука, которая изучает микроскопические объекты, такие как атомы и элементарные частицы. Одним из самых интересных и парадоксальных аспектов квантовой физики является принцип неопределенности. Этот принцип утверждает, что\n"
          ]
        }
      ],
      "source": [
        "# Генерация текста\n",
        "prompt = \"Расскажи интересный факт о квантовой физике.\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
